{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emilygong/miniconda3/envs/paddle_env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import paddle\n",
    "import paddlenlp\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chinese NLP Corpus\n",
    "* https://github.com/SophonPlus/ChineseNlpCorpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess my training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the original dataset\n",
    "weibo_senti = pd.read_csv('weibo_senti_100k.csv')\n",
    "\n",
    "weibo_senti = weibo_senti.rename(columns={'review': 'text'})\n",
    "\n",
    "#define function to swap columns\n",
    "def swap_columns(df, col1, col2):\n",
    "    col_list = list(df.columns)\n",
    "    x, y = col_list.index(col1), col_list.index(col2)\n",
    "    col_list[y], col_list[x] = col_list[x], col_list[y]\n",
    "    df = df[col_list]\n",
    "    return df\n",
    "\n",
    "#swap points and rebounds columns\n",
    "df = swap_columns(weibo_senti, 'label', 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the df into 3 df's - 80% for train, 10% for dev, 10% for test\n",
    "train = df.sample(frac = 0.8, random_state=1)\n",
    "rest = pd.concat([df, train, train]).drop_duplicates(keep=False)\n",
    "dev = rest.sample(frac = 0.5, random_state=1)\n",
    "test = pd.concat([rest, dev, dev]).drop_duplicates(keep=False)\n",
    "# test['label'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(r'train.txt', train.values, fmt='%s')\n",
    "np.savetxt(r'dev.txt', dev.values, fmt='%s')\n",
    "np.savetxt(r'test.txt', test.values, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paddlenlp.datasets import load_dataset\n",
    "\n",
    "def read(data_path):\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            words = line[:-3]\n",
    "            labels = line[-2]\n",
    "            yield {'text': words, 'labels': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = load_dataset(read, data_path='train.txt',lazy=False)\n",
    "dev_ds = load_dataset(read, data_path='dev.txt',lazy=False)\n",
    "test_ds = load_dataset(read, data_path='test.txt',lazy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train example: {'text': '必须严惩！[怒]//@乐妈雒乐: 同意，这个纯属故意杀人！ //@chaton-桐妈:这种人就应该按故意杀人罪判！！', 'labels': '0'}\n",
      "Dev example: {'text': '#风尚志七周年#颁奖盛典暨#LUX?风尚SPARK燃情行动# 期待你的红毯表现！[爱你]//@张伦硕:丝滑芬芳，点燃火花，尽享亲密接触。11月21日在北京，和我一起祝风尚志7周岁生日快乐，见证一场燃情行动，你，准备好了吗？[噢耶][噢耶][噢耶]@风尚志', 'labels': '1'}\n",
      "Test example: {'text': '真的有很多相同之处啊！结拜为兄弟吧！[哈哈]//@厚子林:亲们，视频发啦。回味下泉州的味道。客串主持人好美@南京-老李 @嘉美猫 @温和行者 @苏世独立啦 @繁星满天飞扬 @石泡泡大仙 @Domo-YoYo @大脚丫丫跳芭蕾 @香大菜 @郝浩', 'labels': '1'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Train example:\", train_ds[10])\n",
    "print(\"Dev example:\", dev_ds[10])\n",
    "print(\"Test example:\", test_ds[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-11-21 00:53:25,714] [    INFO]\u001b[0m - We are using <class 'paddlenlp.transformers.ernie.modeling.ErnieForSequenceClassification'> to load 'ernie-3.0-medium-zh'.\u001b[0m\n",
      "\u001b[32m[2022-11-21 00:53:25,716] [    INFO]\u001b[0m - Already cached /home/emilygong/.paddlenlp/models/ernie-3.0-medium-zh/ernie_3.0_medium_zh.pdparams\u001b[0m\n",
      "\u001b[32m[2022-11-21 00:53:33,493] [    INFO]\u001b[0m - We are using <class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'> to load 'ernie-3.0-medium-zh'.\u001b[0m\n",
      "\u001b[32m[2022-11-21 00:53:33,497] [    INFO]\u001b[0m - Already cached /home/emilygong/.paddlenlp/models/ernie-3.0-medium-zh/ernie_3.0_medium_zh_vocab.txt\u001b[0m\n",
      "\u001b[32m[2022-11-21 00:53:33,530] [    INFO]\u001b[0m - tokenizer config file saved in /home/emilygong/.paddlenlp/models/ernie-3.0-medium-zh/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-11-21 00:53:33,531] [    INFO]\u001b[0m - Special tokens file saved in /home/emilygong/.paddlenlp/models/ernie-3.0-medium-zh/special_tokens_map.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_name = \"ernie-3.0-medium-zh\"\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_name, num_classes=len(train_ds.label_list))\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_classes=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_example(example, tokenizer):\n",
    "    tokenized_example = tokenizer(text=example['text'], max_seq_length=128)\n",
    "    # 加上label用于训练\n",
    "    tokenized_example['label'] = [int(example['labels'])]\n",
    "    return tokenized_example\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "trans_func = partial(convert_example, tokenizer=tokenizer)\n",
    "\n",
    "train_ds = train_ds.map(trans_func)\n",
    "dev_ds = dev_ds.map(trans_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paddle.io import DataLoader, BatchSampler\n",
    "from paddlenlp.data import DataCollatorWithPadding\n",
    "\n",
    "collate_fn = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# 定义BatchSampler，选择批大小和是否随机乱序，进行DataLoader\n",
    "train_batch_sampler = BatchSampler(train_ds, batch_size=32, shuffle=True)\n",
    "dev_batch_sampler = BatchSampler(dev_ds, batch_size=64, shuffle=False)\n",
    "train_data_loader = DataLoader(dataset=train_ds, batch_sampler=train_batch_sampler, collate_fn=collate_fn)\n",
    "dev_data_loader = DataLoader(dataset=dev_ds, batch_sampler=dev_batch_sampler, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam优化器、交叉熵损失函数、accuracy评价指标\n",
    "optimizer = paddle.optimizer.AdamW(learning_rate=2e-5, parameters=model.parameters())\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "metric = paddle.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle.nn.functional as F\n",
    "def evaluate(model, metric, data_loader):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(data_loader, start=1):\n",
    "        input_ids, token_type_ids, labels = batch['input_ids'], batch['token_type_ids'], batch['labels']\n",
    "        # 计算模型输出、损失函数值、分类概率值、准确率\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        correct = metric.compute(probs, labels)\n",
    "        metric.update(correct)\n",
    "        acc = metric.accumulate()\n",
    "    print(\"eval accu: %.5f\" % (acc))\n",
    "    model.train()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 10, epoch: 1, batch: 10, loss: 0.63461, accu: 0.58437, speed: 0.08 step/s\n",
      "10 eval accu: 0.73779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-11-14 10:52:35,948] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-11-14 10:52:35,950] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 20, epoch: 1, batch: 20, loss: 0.18690, accu: 0.74006, speed: 0.01 step/s\n",
      "20 "
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "import time\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "# from eval import evaluate\n",
    "# from PaddleNLP.model_zoo.uie.evaluate import evaluate\n",
    "\n",
    "epochs = 5 # 训练轮次\n",
    "ckpt_dir = \"ernie_ckpt\" #训练过程中保存模型参数的文件夹\n",
    "best_acc = 0\n",
    "best_step = 0\n",
    "global_step = 0 #迭代次数\n",
    "tic_train = time.time()\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\n",
    "        input_ids, token_type_ids, labels = batch['input_ids'], batch['token_type_ids'], batch['labels']\n",
    "\n",
    "        # 计算模型输出、损失函数值、分类概率值、准确率\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        correct = metric.compute(probs, labels)\n",
    "        metric.update(correct)\n",
    "        acc = metric.accumulate()\n",
    "\n",
    "        # 每迭代10次，打印损失函数值、准确率、计算速度\n",
    "        global_step += 1\n",
    "        if global_step % 10 == 0:\n",
    "            print(\n",
    "                \"global step %d, epoch: %d, batch: %d, loss: %.5f, accu: %.5f, speed: %.2f step/s\"\n",
    "                % (global_step, epoch, step, loss, acc,\n",
    "                    10 / (time.time() - tic_train)))\n",
    "            tic_train = time.time()\n",
    "        \n",
    "        # 反向梯度回传，更新参数\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.clear_grad()\n",
    "\n",
    "        # 每迭代100次，评估当前训练的模型、保存当前模型参数和分词器的词表等\n",
    "        if global_step % 10 == 0:\n",
    "            save_dir = ckpt_dir\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.makedirs(save_dir)\n",
    "            print(global_step, end=' ')\n",
    "            acc_eval = evaluate(model, metric, dev_data_loader)\n",
    "            if acc_eval > best_acc:\n",
    "                best_acc = acc_eval\n",
    "                best_step = global_step\n",
    "\n",
    "                model.save_pretrained(save_dir)\n",
    "                tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERNIE 3.0-Medium 在weibo_senti的dev集表现 "
     ]
    }
   ],
   "source": [
    "# 加载ERNIR 3.0最佳模型参数\n",
    "params_path = 'ernie_ckpt/model_state.pdparams'\n",
    "state_dict = paddle.load(params_path)\n",
    "model.set_dict(state_dict)\n",
    "\n",
    "# 也可以选择加载预先训练好的模型参数结果查看模型训练结果\n",
    "# model.set_dict(paddle.load('ernie_ckpt_trained/model_state.pdparams'))\n",
    "\n",
    "print('ERNIE 3.0-Medium 在weibo_senti的dev集表现', end=' ')\n",
    "eval_acc = evaluate(model, metric, dev_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def convert_example(example, tokenizer):\n",
    "    tokenized_example = tokenizer(text=example['text'], max_seq_length=128)\n",
    "    # 加上label用于训练\n",
    "    tokenized_example['label'] = [int(example['labels'])]\n",
    "    return tokenized_example\n",
    "\n",
    "trans_func = partial(convert_example, tokenizer=tokenizer)\n",
    "\n",
    "test_ds = test_ds.map(trans_func)\n",
    "\n",
    "# 进行采样组batch\n",
    "collate_fn_test = DataCollatorWithPadding(tokenizer)\n",
    "test_batch_sampler = BatchSampler(test_ds, batch_size=32, shuffle=False)\n",
    "test_data_loader = DataLoader(dataset=test_ds, batch_sampler=test_batch_sampler, collate_fn=collate_fn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型预测分类结果\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "label_map = {0: '负面', 1: '正面'}\n",
    "results = []\n",
    "model.eval()\n",
    "for batch in test_data_loader:\n",
    "    input_ids, token_type_ids = batch['input_ids'], batch['token_type_ids']\n",
    "    logits = model(batch['input_ids'], batch['token_type_ids'])\n",
    "    probs = F.softmax(logits, axis=-1)\n",
    "    idx = paddle.argmax(probs, axis=1).numpy()\n",
    "    idx = idx.tolist()\n",
    "    preds = [label_map[i] for i in idx]\n",
    "    results.extend(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5712"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.count('正面') #positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6286"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.count('负面') #negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERNIE 3.0-Medium 在weibo_senti的test集表现 eval accu: 0.98225\n"
     ]
    }
   ],
   "source": [
    "print('ERNIE 3.0-Medium 在weibo_senti的test集表现', end=' ')\n",
    "eval_acc = evaluate(model, metric, test_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>我是真服了#合肥疫情#</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>啥也不说了，时间真的证明了，武汉真是英雄的城市。</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>如果这个女孩朋友圈是造谣，周处长可以选择报警了；如果她所说属实，周处长真是一位好婆婆。这一邮...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>【昨日#上海新增355例确诊5298例无症状#】上海市卫健委今早（31日）通报：2022年3...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#100万癌症患者的上海生存#过去两个月，在上海这座医疗资源被剧烈争夺的超级大都市，癌症患者...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>“上海封上一周不行吗？”不行！因为上海承载全国乃至全球重要功能#上海为什么不能封城#L中事的...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>本来打了很多字，但后来全部删掉了，因为说太多反而会牵扯很多话题，也不想评论里大家吵起来。现在...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>#全国已有上万名医护人员驰援上海#2020至今，最拉的城市！没有之一！！！！关键这种时候某些...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>没转吉林市丹东市等一系列三四线封城求助是因为我坚信转了也没用，一二线省会城市还会稍微注意以下...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>#抗疫先锋叶财德#他不是网红，但他是真的专家！作为国家卫生健康委应对新冠肺炎疫情社区防控基层...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label\n",
       "0                                          我是真服了#合肥疫情#      0\n",
       "1                             啥也不说了，时间真的证明了，武汉真是英雄的城市。      1\n",
       "2    如果这个女孩朋友圈是造谣，周处长可以选择报警了；如果她所说属实，周处长真是一位好婆婆。这一邮...      0\n",
       "3    【昨日#上海新增355例确诊5298例无症状#】上海市卫健委今早（31日）通报：2022年3...      0\n",
       "4    #100万癌症患者的上海生存#过去两个月，在上海这座医疗资源被剧烈争夺的超级大都市，癌症患者...      1\n",
       "..                                                 ...    ...\n",
       "195  “上海封上一周不行吗？”不行！因为上海承载全国乃至全球重要功能#上海为什么不能封城#L中事的...      0\n",
       "196  本来打了很多字，但后来全部删掉了，因为说太多反而会牵扯很多话题，也不想评论里大家吵起来。现在...      1\n",
       "197  #全国已有上万名医护人员驰援上海#2020至今，最拉的城市！没有之一！！！！关键这种时候某些...      0\n",
       "198  没转吉林市丹东市等一系列三四线封城求助是因为我坚信转了也没用，一二线省会城市还会稍微注意以下...      0\n",
       "199  #抗疫先锋叶财德#他不是网红，但他是真的专家！作为国家卫生健康委应对新冠肺炎疫情社区防控基层...      1\n",
       "\n",
       "[200 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marked = pd.read_csv('new_manual_sa_marked.csv')\n",
    "marked = marked.drop(marked.columns[[0, 1, 2]], axis = 1)\n",
    "marked['sentiment_score'].replace(['negative', 'positive'], [0, 1], inplace=True)\n",
    "marked = marked.rename(columns={'post_text': 'text', 'sentiment_score': 'label'})\n",
    "swap_columns(marked, 'label', 'text')\n",
    "marked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 is too long for the model to predict\n",
    "# 18, 40, 45, 59, 151, 168, 177, 184 contains emoji -> needs some cleaning first\n",
    "# drop_list = [3, 18, 40, 45, 59, 151, 168, 177, 184]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import demoji\n",
    "def clean(context):\n",
    "    try:\n",
    "        context = demoji.replace(context, repl=\"\")\n",
    "    except:\n",
    "        context = context\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "marked['text'] = marked['text'].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>我是真服了#合肥疫情#</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>啥也不说了，时间真的证明了，武汉真是英雄的城市。</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>如果这个女孩朋友圈是造谣，周处长可以选择报警了；如果她所说属实，周处长真是一位好婆婆。这一邮...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#100万癌症患者的上海生存#过去两个月，在上海这座医疗资源被剧烈争夺的超级大都市，癌症患者...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>你的领导（上级）有口臭吗</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>“上海封上一周不行吗？”不行！因为上海承载全国乃至全球重要功能#上海为什么不能封城#L中事的...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>本来打了很多字，但后来全部删掉了，因为说太多反而会牵扯很多话题，也不想评论里大家吵起来。现在...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>#全国已有上万名医护人员驰援上海#2020至今，最拉的城市！没有之一！！！！关键这种时候某些...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>没转吉林市丹东市等一系列三四线封城求助是因为我坚信转了也没用，一二线省会城市还会稍微注意以下...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>#抗疫先锋叶财德#他不是网红，但他是真的专家！作为国家卫生健康委应对新冠肺炎疫情社区防控基层...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label\n",
       "0                                          我是真服了#合肥疫情#      0\n",
       "1                             啥也不说了，时间真的证明了，武汉真是英雄的城市。      1\n",
       "2    如果这个女孩朋友圈是造谣，周处长可以选择报警了；如果她所说属实，周处长真是一位好婆婆。这一邮...      0\n",
       "3    #100万癌症患者的上海生存#过去两个月，在上海这座医疗资源被剧烈争夺的超级大都市，癌症患者...      1\n",
       "4                                         你的领导（上级）有口臭吗      0\n",
       "..                                                 ...    ...\n",
       "194  “上海封上一周不行吗？”不行！因为上海承载全国乃至全球重要功能#上海为什么不能封城#L中事的...      0\n",
       "195  本来打了很多字，但后来全部删掉了，因为说太多反而会牵扯很多话题，也不想评论里大家吵起来。现在...      1\n",
       "196  #全国已有上万名医护人员驰援上海#2020至今，最拉的城市！没有之一！！！！关键这种时候某些...      0\n",
       "197  没转吉林市丹东市等一系列三四线封城求助是因为我坚信转了也没用，一二线省会城市还会稍微注意以下...      0\n",
       "198  #抗疫先锋叶财德#他不是网红，但他是真的专家！作为国家卫生健康委应对新冠肺炎疫情社区防控基层...      1\n",
       "\n",
       "[199 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marked = marked.drop(3).reset_index(drop=True)\n",
    "marked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'【#吉林新增本土确诊1730例##吉林新增本土无症状1244例#】4月1日0-24时，吉林全省新增本地确诊病例1730例(轻型1720例、普通型10例），其中长春市1544例（含19例无症状感染者转为确诊病例）、吉林市178例、四平市6例（含1例无症状感染者转为确诊病例）、白城市2例；新增本地无症状感染者1244例，其中长春市894例、吉林市349例、四平市1例。以上感染者均已转运至定点医疗机构隔离治疗，对以上人员的密切接触者、次密切接触者均已开展追踪排查，并落实管控措施，对其生活和工作场所进行了终末消毒。'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marked.iat[6, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(r'actual_test.txt', marked.values, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_test_ds = load_dataset(read, data_path='actual_test.txt',lazy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def convert_example(example, tokenizer):\n",
    "    tokenized_example = tokenizer(text=example['text'], max_seq_length=128)\n",
    "    # 加上label用于训练\n",
    "    tokenized_example['label'] = [int(example['labels'])]\n",
    "    return tokenized_example\n",
    "\n",
    "trans_func = partial(convert_example, tokenizer=tokenizer)\n",
    "\n",
    "actual_test_ds = actual_test_ds.map(trans_func)\n",
    "\n",
    "# 进行采样组batch\n",
    "collate_fn_test = DataCollatorWithPadding(tokenizer)\n",
    "test_batch_sampler = BatchSampler(test_ds, batch_size=32, shuffle=False)\n",
    "actual_test_data_loader = DataLoader(dataset=actual_test_ds, batch_sampler=test_batch_sampler, collate_fn=collate_fn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13,\n",
       " 26,\n",
       " 76,\n",
       " 197,\n",
       " 14,\n",
       " 129,\n",
       " 235,\n",
       " 99,\n",
       " 192,\n",
       " 142,\n",
       " 248,\n",
       " 14,\n",
       " 165,\n",
       " 130,\n",
       " 36,\n",
       " 414,\n",
       " 34,\n",
       " 364,\n",
       " 12,\n",
       " 37,\n",
       " 619,\n",
       " 147,\n",
       " 25,\n",
       " 460,\n",
       " 245,\n",
       " 51,\n",
       " 510,\n",
       " 22,\n",
       " 32,\n",
       " 304,\n",
       " 97,\n",
       " 127,\n",
       " 75,\n",
       " 43,\n",
       " 174,\n",
       " 148,\n",
       " 251,\n",
       " 233,\n",
       " 50,\n",
       " 69,\n",
       " 170,\n",
       " 684,\n",
       " 19,\n",
       " 43,\n",
       " 87,\n",
       " 423,\n",
       " 92,\n",
       " 19,\n",
       " 473,\n",
       " 3,\n",
       " 29,\n",
       " 173,\n",
       " 127,\n",
       " 31,\n",
       " 881,\n",
       " 192,\n",
       " 50,\n",
       " 177,\n",
       " 149,\n",
       " 375,\n",
       " 10,\n",
       " 25,\n",
       " 512,\n",
       " 290,\n",
       " 184,\n",
       " 359,\n",
       " 359,\n",
       " 435,\n",
       " 15,\n",
       " 569,\n",
       " 826,\n",
       " 233,\n",
       " 12,\n",
       " 66,\n",
       " 10,\n",
       " 25,\n",
       " 45,\n",
       " 35,\n",
       " 20,\n",
       " 881,\n",
       " 328,\n",
       " 62,\n",
       " 358,\n",
       " 272,\n",
       " 52,\n",
       " 113,\n",
       " 94,\n",
       " 345,\n",
       " 190,\n",
       " 144,\n",
       " 108,\n",
       " 322,\n",
       " 42,\n",
       " 43,\n",
       " 247,\n",
       " 122,\n",
       " 156,\n",
       " 39,\n",
       " 8,\n",
       " 20,\n",
       " 193,\n",
       " 44,\n",
       " 169,\n",
       " 143,\n",
       " 38,\n",
       " 122,\n",
       " 451,\n",
       " 39,\n",
       " 571,\n",
       " 338,\n",
       " 238,\n",
       " 96,\n",
       " 37,\n",
       " 119,\n",
       " 33,\n",
       " 125,\n",
       " 41,\n",
       " 62,\n",
       " 914,\n",
       " 104,\n",
       " 30,\n",
       " 376,\n",
       " 31,\n",
       " 21,\n",
       " 573,\n",
       " 42,\n",
       " 315,\n",
       " 22,\n",
       " 51,\n",
       " 229,\n",
       " 62,\n",
       " 262,\n",
       " 158,\n",
       " 103,\n",
       " 159,\n",
       " 433,\n",
       " 117,\n",
       " 210,\n",
       " 74,\n",
       " 442,\n",
       " 97,\n",
       " 12,\n",
       " 382,\n",
       " 529,\n",
       " 322,\n",
       " 137,\n",
       " 272,\n",
       " 491,\n",
       " 629,\n",
       " 655,\n",
       " 9,\n",
       " 213,\n",
       " 419,\n",
       " 93,\n",
       " 150,\n",
       " 8,\n",
       " 259,\n",
       " 205,\n",
       " 53,\n",
       " 382,\n",
       " 130,\n",
       " 163,\n",
       " 51,\n",
       " 313,\n",
       " 390,\n",
       " 540,\n",
       " 122,\n",
       " 35,\n",
       " 77,\n",
       " 50,\n",
       " 54,\n",
       " 19,\n",
       " 18,\n",
       " 310,\n",
       " 17,\n",
       " 13,\n",
       " 55,\n",
       " 61,\n",
       " 110,\n",
       " 157,\n",
       " 60,\n",
       " 153,\n",
       " 309,\n",
       " 562,\n",
       " 6,\n",
       " 169,\n",
       " 301,\n",
       " 43,\n",
       " 19,\n",
       " 53,\n",
       " 600,\n",
       " 236,\n",
       " 196,\n",
       " 80,\n",
       " 52,\n",
       " 196,\n",
       " 88,\n",
       " 130,\n",
       " 254]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_val =[]\n",
    "for i in actual_test_ds:\n",
    "    max_val.append(len(i['input_ids']))\n",
    "max_val\n",
    "# for i in range(len(max_val)):\n",
    "#     if max_val[i] == 39979:\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PretrainedTokenizer(name_or_path='', vocab_size=39979, model_max_len=2048, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERNIE 3.0-Medium 在my weibo datatest的表现 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/emilygong/miniconda3/envs/paddle_env/lib/python3.9/threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/emilygong/miniconda3/envs/paddle_env/lib/python3.9/threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/emilygong/miniconda3/envs/paddle_env/lib/python3.9/site-packages/paddle/fluid/dataloader/dataloader_iter.py\", line 217, in _thread_loop\n",
      "    batch = self._dataset_fetcher.fetch(indices,\n",
      "  File \"/home/emilygong/miniconda3/envs/paddle_env/lib/python3.9/site-packages/paddle/fluid/dataloader/fetcher.py\", line 121, in fetch\n",
      "    data.append(self.dataset[idx])\n",
      "  File \"/home/emilygong/miniconda3/envs/paddle_env/lib/python3.9/site-packages/paddlenlp/datasets/dataset.py\", line 276, in __getitem__\n",
      "    self.new_data[idx]\n",
      "IndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "# 加载ERNIR 3.0最佳模型参数\n",
    "params_path = 'ernie_ckpt/model_state.pdparams'\n",
    "state_dict = paddle.load(params_path)\n",
    "model.set_dict(state_dict)\n",
    "\n",
    "print('ERNIE 3.0-Medium 在my weibo datatest的表现', end=' ')\n",
    "actual_eval_acc = evaluate(model, metric, actual_test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paddle_env",
   "language": "python",
   "name": "paddle_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
