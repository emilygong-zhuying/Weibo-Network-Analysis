{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emilygong/miniconda3/envs/paddle_env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import paddle\n",
    "import paddlenlp\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chinese NLP Corpus\n",
    "* https://github.com/SophonPlus/ChineseNlpCorpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess my training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the original dataset\n",
    "weibo_senti = pd.read_csv('weibo_senti_100k.csv')\n",
    "\n",
    "weibo_senti = weibo_senti.rename(columns={'review': 'text'})\n",
    "\n",
    "#define function to swap columns\n",
    "def swap_columns(df, col1, col2):\n",
    "    col_list = list(df.columns)\n",
    "    x, y = col_list.index(col1), col_list.index(col2)\n",
    "    col_list[y], col_list[x] = col_list[x], col_list[y]\n",
    "    df = df[col_list]\n",
    "    return df\n",
    "\n",
    "#swap points and rebounds columns\n",
    "df = swap_columns(weibo_senti, 'label', 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the df into 3 df's - 80% for train, 10% for dev, 10% for test\n",
    "train = df.sample(frac = 0.8, random_state=1)\n",
    "rest = pd.concat([df, train, train]).drop_duplicates(keep=False)\n",
    "dev = rest.sample(frac = 0.5, random_state=1)\n",
    "test = pd.concat([rest, dev, dev]).drop_duplicates(keep=False)\n",
    "# test['label'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>【霍思燕剖腹产下“小江江” 老公落泪】今晨9时霍思燕产下一名男婴，宝宝重8斤3两，母子平安。...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>顶风作案!明知有雨，但实在舍不得半个月前就定好的票!40大洋一张呢，3个人120大洋呢![嘻...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>偶爱土豆[哈哈]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>//@爱旅游爱赣州: 中央四套出品，[good]展示赣州客家人文历史，将拍十二集，九月左右播...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>挺好玩儿的，过年更充实了。[太开心] //@母其弥雅:初一我?一起吃素吧~?得不要吃稀?，洗...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59950</th>\n",
       "      <td>我刚认真想了想，主要是因为市场部4个家伙全部热恋中。。。难怪这么抽疯。。。产品童鞋真善解人意...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59960</th>\n",
       "      <td>我靠！[鼓掌]  //@必须叫我姐姐:巴萨、巴萨、巴萨，我在巴萨的主场包箱里看过球。有香槟、...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59961</th>\n",
       "      <td>大家应该大部分都回到自己的工作岗位或开始新学年了对吧？有没有感觉到假期后感觉闷闷不乐[失望]...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59963</th>\n",
       "      <td>对@云卷云舒的胡言乱语 说[话筒]：祝愿帅哥图图：生日快乐！[蛋糕][礼物][鼓掌]早日找到...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59971</th>\n",
       "      <td>[哈哈] //@凌燕爱猪蹄儿_YC:[汗]我等着见证第八百对~[偷笑]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5926 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "9      【霍思燕剖腹产下“小江江” 老公落泪】今晨9时霍思燕产下一名男婴，宝宝重8斤3两，母子平安。...      1\n",
       "59     顶风作案!明知有雨，但实在舍不得半个月前就定好的票!40大洋一张呢，3个人120大洋呢![嘻...      1\n",
       "60                                              偶爱土豆[哈哈]      1\n",
       "78     //@爱旅游爱赣州: 中央四套出品，[good]展示赣州客家人文历史，将拍十二集，九月左右播...      1\n",
       "80     挺好玩儿的，过年更充实了。[太开心] //@母其弥雅:初一我?一起吃素吧~?得不要吃稀?，洗...      1\n",
       "...                                                  ...    ...\n",
       "59950  我刚认真想了想，主要是因为市场部4个家伙全部热恋中。。。难怪这么抽疯。。。产品童鞋真善解人意...      1\n",
       "59960  我靠！[鼓掌]  //@必须叫我姐姐:巴萨、巴萨、巴萨，我在巴萨的主场包箱里看过球。有香槟、...      1\n",
       "59961  大家应该大部分都回到自己的工作岗位或开始新学年了对吧？有没有感觉到假期后感觉闷闷不乐[失望]...      1\n",
       "59963  对@云卷云舒的胡言乱语 说[话筒]：祝愿帅哥图图：生日快乐！[蛋糕][礼物][鼓掌]早日找到...      1\n",
       "59971                [哈哈] //@凌燕爱猪蹄儿_YC:[汗]我等着见证第八百对~[偷笑]      1\n",
       "\n",
       "[5926 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[test.label == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(r'train.txt', train.values, fmt='%s')\n",
    "np.savetxt(r'dev.txt', dev.values, fmt='%s')\n",
    "np.savetxt(r'test.txt', test.values, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paddlenlp.datasets import load_dataset\n",
    "\n",
    "def read(data_path):\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        # 跳过列名\n",
    "        next(f)\n",
    "        for line in f:\n",
    "            words = line[:-3]\n",
    "            labels = line[-2]\n",
    "            yield {'text': words, 'labels': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = load_dataset(read, data_path='train.txt',lazy=False)\n",
    "dev_ds = load_dataset(read, data_path='dev.txt',lazy=False)\n",
    "test_ds = load_dataset(read, data_path='test.txt',lazy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train example: {'text': '回来晚忘记给酒店打电话了[泪]只能加钱住“ 行政大床房 ”[汗]@--怡然自乐 @小尤爸爸 @猪露头 @生于静默 @滴沥波罗 @羊和猪的生活意见 @雏菊花开的围脖 @最左边那只', 'labels': '0'}\n",
      "Dev example: {'text': '结果我没下来，你下来了[衰]', 'labels': '0'}\n",
      "Test example: {'text': '晚饭开始啦！[抱抱] 这是什么菜？竞猜竞猜！！', 'labels': '1'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Train example:\", train_ds[10])\n",
    "print(\"Dev example:\", dev_ds[10])\n",
    "print(\"Test example:\", test_ds[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-11-15 09:54:39,143] [    INFO]\u001b[0m - We are using <class 'paddlenlp.transformers.ernie.modeling.ErnieForSequenceClassification'> to load 'ernie-3.0-medium-zh'.\u001b[0m\n",
      "\u001b[32m[2022-11-15 09:54:39,147] [    INFO]\u001b[0m - Already cached /home/emilygong/.paddlenlp/models/ernie-3.0-medium-zh/ernie_3.0_medium_zh.pdparams\u001b[0m\n",
      "\u001b[32m[2022-11-15 09:54:46,975] [    INFO]\u001b[0m - We are using <class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'> to load 'ernie-3.0-medium-zh'.\u001b[0m\n",
      "\u001b[32m[2022-11-15 09:54:46,980] [    INFO]\u001b[0m - Already cached /home/emilygong/.paddlenlp/models/ernie-3.0-medium-zh/ernie_3.0_medium_zh_vocab.txt\u001b[0m\n",
      "\u001b[32m[2022-11-15 09:54:47,011] [    INFO]\u001b[0m - tokenizer config file saved in /home/emilygong/.paddlenlp/models/ernie-3.0-medium-zh/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-11-15 09:54:47,013] [    INFO]\u001b[0m - Special tokens file saved in /home/emilygong/.paddlenlp/models/ernie-3.0-medium-zh/special_tokens_map.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_name = \"ernie-3.0-medium-zh\"\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_name, num_classes=len(train_ds.label_list))\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_classes=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_example(example, tokenizer):\n",
    "    tokenized_example = tokenizer(text=example['text'], max_seq_length=128)\n",
    "    # 加上label用于训练\n",
    "    tokenized_example['label'] = [int(example['labels'])]\n",
    "    return tokenized_example\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "trans_func = partial(convert_example, tokenizer=tokenizer)\n",
    "\n",
    "train_ds = train_ds.map(trans_func)\n",
    "dev_ds = dev_ds.map(trans_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from paddlenlp.datasets import MapDataset\n",
    "\n",
    "# train_list = convert(train_ds, tokenizer)\n",
    "# train_ds = MapDataset(train_list)\n",
    "# dev_list = convert(dev_ds, tokenizer)\n",
    "# dev_ds = MapDataset(dev_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paddle.io import DataLoader, BatchSampler\n",
    "from paddlenlp.data import DataCollatorWithPadding\n",
    "\n",
    "collate_fn = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# 定义BatchSampler，选择批大小和是否随机乱序，进行DataLoader\n",
    "train_batch_sampler = BatchSampler(train_ds, batch_size=32, shuffle=True)\n",
    "dev_batch_sampler = BatchSampler(dev_ds, batch_size=64, shuffle=False)\n",
    "train_data_loader = DataLoader(dataset=train_ds, batch_sampler=train_batch_sampler, collate_fn=collate_fn)\n",
    "dev_data_loader = DataLoader(dataset=dev_ds, batch_sampler=dev_batch_sampler, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport functools\\nimport numpy as np\\n\\nfrom paddle.io import DataLoader, BatchSampler\\nfrom paddlenlp.data import DataCollatorWithPadding\\n\\n# 数据预处理函数，利用分词器将文本转化为整数序列\\ndef preprocess_function(examples, tokenizer, max_seq_length, is_test=False):\\n    result = tokenizer(text=examples[\"text\"], max_seq_len=max_seq_length)\\n    if not is_test:\\n        result[\"labels\"] = examples[\"label\"]\\n    return result\\n\\ntrans_func = functools.partial(preprocess_function, tokenizer=tokenizer, max_seq_length=128)\\ntrain_ds = train_ds.map(trans_func)\\ndev_ds = dev_ds.map(trans_func)\\n\\n# collate_fn函数构造，将不同长度序列充到批中数据的最大长度，再将数据堆叠\\ncollate_fn = DataCollatorWithPadding(tokenizer)\\n\\n# 定义BatchSampler，选择批大小和是否随机乱序，进行DataLoader\\ntrain_batch_sampler = BatchSampler(train_ds, batch_size=32, shuffle=True)\\ndev_batch_sampler = BatchSampler(dev_ds, batch_size=64, shuffle=False)\\ntrain_data_loader = DataLoader(dataset=train_ds, batch_sampler=train_batch_sampler, collate_fn=collate_fn)\\ndev_data_loader = DataLoader(dataset=dev_ds, batch_sampler=dev_batch_sampler, collate_fn=collate_fn)\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import functools\n",
    "import numpy as np\n",
    "\n",
    "from paddle.io import DataLoader, BatchSampler\n",
    "from paddlenlp.data import DataCollatorWithPadding\n",
    "\n",
    "# 数据预处理函数，利用分词器将文本转化为整数序列\n",
    "def preprocess_function(examples, tokenizer, max_seq_length, is_test=False):\n",
    "    result = tokenizer(text=examples[\"text\"], max_seq_len=max_seq_length)\n",
    "    if not is_test:\n",
    "        result[\"labels\"] = examples[\"label\"]\n",
    "    return result\n",
    "\n",
    "trans_func = functools.partial(preprocess_function, tokenizer=tokenizer, max_seq_length=128)\n",
    "train_ds = train_ds.map(trans_func)\n",
    "dev_ds = dev_ds.map(trans_func)\n",
    "\n",
    "# collate_fn函数构造，将不同长度序列充到批中数据的最大长度，再将数据堆叠\n",
    "collate_fn = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# 定义BatchSampler，选择批大小和是否随机乱序，进行DataLoader\n",
    "train_batch_sampler = BatchSampler(train_ds, batch_size=32, shuffle=True)\n",
    "dev_batch_sampler = BatchSampler(dev_ds, batch_size=64, shuffle=False)\n",
    "train_data_loader = DataLoader(dataset=train_ds, batch_sampler=train_batch_sampler, collate_fn=collate_fn)\n",
    "dev_data_loader = DataLoader(dataset=dev_ds, batch_sampler=dev_batch_sampler, collate_fn=collate_fn)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam优化器、交叉熵损失函数、accuracy评价指标\n",
    "optimizer = paddle.optimizer.AdamW(learning_rate=2e-5, parameters=model.parameters())\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "metric = paddle.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle.nn.functional as F\n",
    "def evaluate(model, metric, data_loader):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(data_loader, start=1):\n",
    "        input_ids, token_type_ids, labels = batch['input_ids'], batch['token_type_ids'], batch['labels']\n",
    "\n",
    "        # 计算模型输出、损失函数值、分类概率值、准确率\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        correct = metric.compute(probs, labels)\n",
    "        metric.update(correct)\n",
    "        acc = metric.accumulate()\n",
    "    print(\"eval accu: %.5f\" % (acc))\n",
    "    model.train()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 10, epoch: 1, batch: 10, loss: 0.63461, accu: 0.58437, speed: 0.08 step/s\n",
      "10 eval accu: 0.73779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-11-14 10:52:35,948] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-11-14 10:52:35,950] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 20, epoch: 1, batch: 20, loss: 0.18690, accu: 0.74006, speed: 0.01 step/s\n",
      "20 "
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "import time\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "# from eval import evaluate\n",
    "# from PaddleNLP.model_zoo.uie.evaluate import evaluate\n",
    "\n",
    "epochs = 5 # 训练轮次\n",
    "ckpt_dir = \"ernie_ckpt\" #训练过程中保存模型参数的文件夹\n",
    "best_acc = 0\n",
    "best_step = 0\n",
    "global_step = 0 #迭代次数\n",
    "tic_train = time.time()\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\n",
    "        input_ids, token_type_ids, labels = batch['input_ids'], batch['token_type_ids'], batch['labels']\n",
    "\n",
    "        # 计算模型输出、损失函数值、分类概率值、准确率\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        correct = metric.compute(probs, labels)\n",
    "        metric.update(correct)\n",
    "        acc = metric.accumulate()\n",
    "\n",
    "        # 每迭代10次，打印损失函数值、准确率、计算速度\n",
    "        global_step += 1\n",
    "        if global_step % 10 == 0:\n",
    "            print(\n",
    "                \"global step %d, epoch: %d, batch: %d, loss: %.5f, accu: %.5f, speed: %.2f step/s\"\n",
    "                % (global_step, epoch, step, loss, acc,\n",
    "                    10 / (time.time() - tic_train)))\n",
    "            tic_train = time.time()\n",
    "        \n",
    "        # 反向梯度回传，更新参数\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.clear_grad()\n",
    "\n",
    "        # 每迭代100次，评估当前训练的模型、保存当前模型参数和分词器的词表等\n",
    "        if global_step % 10 == 0:\n",
    "            save_dir = ckpt_dir\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.makedirs(save_dir)\n",
    "            print(global_step, end=' ')\n",
    "            acc_eval = evaluate(model, metric, dev_data_loader)\n",
    "            if acc_eval > best_acc:\n",
    "                best_acc = acc_eval\n",
    "                best_step = global_step\n",
    "\n",
    "                model.save_pretrained(save_dir)\n",
    "                tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERNIE 3.0-Medium 在weibo_senti的dev集表现 eval accu: 0.98291\n"
     ]
    }
   ],
   "source": [
    "# 加载ERNIR 3.0最佳模型参数\n",
    "params_path = 'ernie_ckpt/model_state.pdparams'\n",
    "state_dict = paddle.load(params_path)\n",
    "model.set_dict(state_dict)\n",
    "\n",
    "# 也可以选择加载预先训练好的模型参数结果查看模型训练结果\n",
    "# model.set_dict(paddle.load('ernie_ckpt_trained/model_state.pdparams'))\n",
    "\n",
    "print('ERNIE 3.0-Medium 在weibo_senti的dev集表现', end=' ')\n",
    "eval_acc = evaluate(model, metric, dev_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def convert_example(example, tokenizer):\n",
    "    tokenized_example = tokenizer(text=example['text'], max_seq_length=128)\n",
    "    # 加上label用于训练\n",
    "    tokenized_example['label'] = [int(example['labels'])]\n",
    "    return tokenized_example\n",
    "\n",
    "trans_func = partial(convert_example, tokenizer=tokenizer)\n",
    "\n",
    "test_ds = test_ds.map(trans_func)\n",
    "\n",
    "# 进行采样组batch\n",
    "collate_fn_test = DataCollatorWithPadding(tokenizer)\n",
    "test_batch_sampler = BatchSampler(test_ds, batch_size=32, shuffle=False)\n",
    "test_data_loader = DataLoader(dataset=test_ds, batch_sampler=test_batch_sampler, collate_fn=collate_fn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型预测分类结果\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "label_map = {0: '负面', 1: '正面'}\n",
    "results = []\n",
    "model.eval()\n",
    "for batch in test_data_loader:\n",
    "    input_ids, token_type_ids = batch['input_ids'], batch['token_type_ids']\n",
    "    logits = model(batch['input_ids'], batch['token_type_ids'])\n",
    "    probs = F.softmax(logits, axis=-1)\n",
    "    idx = paddle.argmax(probs, axis=1).numpy()\n",
    "    idx = idx.tolist()\n",
    "    preds = [label_map[i] for i in idx]\n",
    "    results.extend(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2022-11-15 10:16:05,592 download.py:117] unique_endpoints {''}\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqid\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, pred \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(results):\n\u001b[0;32m---> 10\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(\u001b[43mtest_ds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqid\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mtest_ds[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mpred\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/paddle_env/lib/python3.9/site-packages/paddlenlp/datasets/dataset.py:277\u001b[0m, in \u001b[0;36mMapDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;03m    Basic function of `MapDataset` to get sample from dataset with a given \u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;124;03m    index.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform(\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_data[idx]\n\u001b[0;32m--> 277\u001b[0m     ) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_pipline \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# 存储ChnSentiCorp预测结果  \n",
    "test_ds = load_dataset(\"chnsenticorp\", splits=[\"test\"]) \n",
    "\n",
    "res_dir = \"./results\"\n",
    "if not os.path.exists(res_dir):\n",
    "    os.makedirs(res_dir)\n",
    "with open(os.path.join(res_dir, \"ChnSentiCorp.tsv\"), 'w', encoding=\"utf8\") as f:\n",
    "    f.write(\"qid\\ttext\\tprediction\\n\")\n",
    "    for i, pred in enumerate(results):\n",
    "        f.write(test_ds[i]['qid']+\"\\t\"+test_ds[i]['text']+\"\\t\"+pred+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11998"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.count('正面')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11998"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paddle_env",
   "language": "python",
   "name": "paddle_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
